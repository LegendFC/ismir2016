@article{Abdel-Hamid2014,
  title={Convolutional neural networks for speech recognition},
  author={Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  journal={Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
  volume={22},
  number={10},
  pages={1533--1545},
  year={2014},
  publisher={IEEE}
}

@inproceedings{Anden2015,
abstract = {We introduce the joint time-frequency scattering transform, a time shift invariant descriptor of time-frequency structure for audio classification. It is obtained by applying a two-dimensional wavelet transform in time and log-frequency to a time-frequency wavelet scalogram. We show that this descriptor successfully characterizes complex time-frequency phenomena such as time-varying filters and frequency modulated excitations. State-of-the-art results are achieved for signal reconstruction and phone segment classification on the TIMIT dataset.},
archivePrefix = {arXiv},
arxivId = {1512.02125},
author = {And{\'{e}}n, Joakim and Lostanlen, Vincent and Mallat, St{\'{e}}phane},
booktitle = {Machine Learning for Signal Processing},
doi = {10.1109/MLSP.2015.7324385},
eprint = {1512.02125},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/e8ca21f1ef9b9dbe3f51eccf3d5afd24d08df1c3.pdf:pdf},
isbn = {9781467374545},
title = {{Joint Time-Frequency Scattering for Audio Classification}},
url = {http://arxiv.org/abs/1512.02125
http://dx.doi.org/10.1109/MLSP.2015.7324385},
year = {2015}
}


@article{Bittner2014,
abstract = {We introduce MedleyDB: a dataset of annotated, royalty-free multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research.},
author = {Bittner, Rachel and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan},
journal = {International Society for Music Information Retrieval Conference},
title = {MedleyDB: a multitrack dataset for annotation-intensive MIR research},
url = {http://marl.smusic.nyu.edu/medleydb{\_}webfiles/bittner{\_}medleydb{\_}ismir2014.pdf},
year = {2014}
}

@article{Hamel2012,
abstract = {Low-level aspects of music audio such as timbre, loud- ness and pitch, can be relatively well modelled by features extracted from short-time windows. Higher-level aspects such as melody, harmony, phrasing and rhythm, on the other hand, are salient only at larger timescales and re- quire a better representation of time dynamics. For var- ious music information retrieval tasks, one would bene?t from modelling both low and high level aspects in a uni- ?ed feature extraction framework. By combining adaptive features computed at different timescales, short-timescale events are put in context by detecting longer timescale fea- tures. In this paper, we describe a method to obtain such multi-scale features and evaluate its effectiveness for auto- matic tag annotation.},
author = {Hamel, Philippe and Bengio, Yoshua and Eck, Douglas},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/18e4b042f9134a9d963ecc9fa0e17703e06572cc.pdf:pdf},
isbn = {9789727521449},
journal = {Proceedings of the 13th International Society for Musical Information Retrieval Conference},
number = {Ismir},
pages = {553--558},
title = {{Building musically-relevant audio features through multiple timescale representations}},
url = {http://ismir2012.ismir.net/event/papers/553{\_}ISMIR{\_}2012.pdf},
year = {2012}
}

@article{Joder2009,
abstract = {Nowadays, it appears essential to design automatic indexing tools which provide meaningful and efficient means to describe the musical audio content. There is in fact a growing interest for music information retrieval (MIR) applications amongst which the most popular are related to music similarity retrieval, artist identification, musical genre or instrument recognition. Current MIR-related classification systems usually do not take into account the mid-term temporal properties of the signal (over several frames) and lie on the assumption that the observations of the features in different frames are statistically independent. The aim of this paper is to demonstrate the usefulness of the information carried by the evolution of these characteristics over time. To that purpose, we propose a number of methods for early and late temporal integration and provide an in-depth experimental study on their interest for the task of musical instrument recognition on solo musical phrases. In particular, the impact of the time horizon over which the temporal integration is performed will be assessed both for fixed and variable frame length analysis. Also, a number of proposed alignment kernels will be used for late temporal integration. For all experiments, the results are compared to a state of the art musical instrument recognition system.},
author = {Joder, Cyril and Essid, Slim and Richard, Ga{\"{e}}l},
doi = {10.1109/TASL.2008.2007613},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Alignment kernels,Audio classification,Music information retrieval (MIR),Musical instrument recognition,Support vector machine (SVM),Temporal feature integration},
number = {1},
pages = {174--186},
title = {Temporal integration for audio classification with application to musical instrument classification},
url = {http://biblio.telecom-paristech.fr/cgi-bin/download.cgi?id=8585},
volume = {17},
year = {2009}
}

@article{Kereliuk2015,
abstract = {An adversary is essentially an algorithm intent on making a classification system perform in some particular way given an input, e.g., increase the probability of a false negative. Recent work builds adversaries for deep learning systems applied to image object recognition, which exploits the parameters of the system to find the minimal perturbation of the input image such that the network misclassifies it with high confidence. We adapt this approach to construct and deploy an adversary of deep learning systems applied to music content analysis. In our case, however, the input to the systems is magnitude spectral frames, which requires special care in order to produce valid input audio signals from network-derived perturbations. For two different train-test partitionings of two benchmark datasets, and two different deep architectures, we find that this adversary is very effective in defeating the resulting systems. We find the convolutional networks are more robust, however, compared with systems based on a majority vote over individually classified audio frames. Furthermore, we integrate the adversary into the training of new deep systems, but do not find that this improves their resilience against the same adversary.},
archivePrefix = {arXiv},
arxivId = {1507.04761},
author = {Kereliuk, Corey and Sturm, Bob L. and Larsen, Jan},
doi = {10.1109/TMM.2015.2478068},
eprint = {1507.04761},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/01005e6cff03ae253c088741b53ffe4a7280450e.pdf:pdf},
issn = {15209210},
journal = {IEEE Transactions on Multimedia},
keywords = {AEA-MIR content-based processing and music information retrieval,deep learning},
number = {11},
pages = {2059--2071},
title = {{Deep Learning and Music Adversaries}},
url = {http://www2.imm.dtu.dk/pubdb/views/edoc{\_}download.php/6904/pdf/imm6904.pdf},
volume = {17},
year = {2015}
}

@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v5},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v5},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/cf5cf758f1b00214d70e34f8a57813557dc03e17.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--13},
title = {{Adam: a Method for Stochastic Optimization}},
url = {http://arxiv.org/pdf/1412.6980v8.pdf},
year = {2015}
}

@article{Li2015,
abstract = {Traditional methods to tackle many music information retrieval tasks typically follow a two-step architecture: feature engineering followed by a simple learning algorithm. In these "shallow" architectures, feature engineering and learning are typically disjoint and unrelated. Additionally, feature engineering is difficult, and typically depends on extensive domain expertise. In this paper, we present an application of convolutional neural networks for the task of automatic musical instrument identification. In this model, feature extraction and learning algorithms are trained together in an end-to-end fashion. We show that a convolutional neural network trained on raw audio can achieve performance surpassing traditional methods that rely on hand-crafted features.},
archivePrefix = {arXiv},
arxivId = {1511.05520},
author = {Li, Peter and Qian, Jiyuan and Wang, Tian},
eprint = {1511.05520},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/fedeeda2182eef200a4ac9af9caf95708bd0ecca.pdf:pdf},
title = {{Automatic Instrument Recognition in Polyphonic Music Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1511.05520},
year = {2015}
}

@misc{McFee2015-librosa,
  author       = {Brian McFee and
                  Matt McVicar and
                  Colin Raffel and
                  Dawen Liang and
                  Oriol Nieto and
                  Eric Battenberg and
                  Josh Moore and
                  Dan Ellis and
                  Ryuichi Yamamoto and
                  Rachel Bittner and
                  Douglas Repetto and
                  Petr Viktorin and
                  Jo�o Felipe Santos and
                  Adrian Holovaty},
  title        = {librosa: 0.4.1. Zenodo. 10.5281/zenodo.18369},
  month        = oct,
  year         = 2015,
  doi          = {10.5281/zenodo.32193},
  url          = {http://dx.doi.org/10.5281/zenodo.32193}
}

@article{Mcfee2015-muda,
author = {Mcfee, Brian and Humphrey, Eric J and Bello, Juan P},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/71aa6ba50f44c81060db67c7cb237d2cc72be1f1.pdf:pdf},
pages = {248--254},
title = {{A software framework for musical data augmentation}},
url = {https://bmcfee.github.io/papers/ismir2015{\_}augmentation.pdf},
year = {2015}
}

