@article{Abdel-Hamid2014,
  title={Convolutional neural networks for speech recognition},
  author={Abdel-Hamid, Ossama and Mohamed, Abdel-rahman and Jiang, Hui and Deng, Li and Penn, Gerald and Yu, Dong},
  journal={Audio, Speech, and Language Processing, IEEE/ACM Transactions on},
  volume={22},
  number={10},
  pages={1533--1545},
  year={2014},
  publisher={IEEE}
}

@article{Bittner2014,
abstract = {We introduce MedleyDB: a dataset of annotated, royalty-free multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research.},
author = {Bittner, Rachel and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan},
journal = {International Society for Music Information Retrieval Conference},
title = {MedleyDB: a multitrack dataset for annotation-intensive MIR research},
url = {http://marl.smusic.nyu.edu/medleydb{\_}webfiles/bittner{\_}medleydb{\_}ismir2014.pdf},
year = {2014}
}

@article{Joder2009,
abstract = {Nowadays, it appears essential to design automatic indexing tools which provide meaningful and efficient means to describe the musical audio content. There is in fact a growing interest for music information retrieval (MIR) applications amongst which the most popular are related to music similarity retrieval, artist identification, musical genre or instrument recognition. Current MIR-related classification systems usually do not take into account the mid-term temporal properties of the signal (over several frames) and lie on the assumption that the observations of the features in different frames are statistically independent. The aim of this paper is to demonstrate the usefulness of the information carried by the evolution of these characteristics over time. To that purpose, we propose a number of methods for early and late temporal integration and provide an in-depth experimental study on their interest for the task of musical instrument recognition on solo musical phrases. In particular, the impact of the time horizon over which the temporal integration is performed will be assessed both for fixed and variable frame length analysis. Also, a number of proposed alignment kernels will be used for late temporal integration. For all experiments, the results are compared to a state of the art musical instrument recognition system.},
author = {Joder, Cyril and Essid, Slim and Richard, Ga{\"{e}}l},
doi = {10.1109/TASL.2008.2007613},
issn = {15587916},
journal = {IEEE Transactions on Audio, Speech and Language Processing},
keywords = {Alignment kernels,Audio classification,Music information retrieval (MIR),Musical instrument recognition,Support vector machine (SVM),Temporal feature integration},
number = {1},
pages = {174--186},
title = {Temporal integration for audio classification with application to musical instrument classification},
url = {http://biblio.telecom-paristech.fr/cgi-bin/download.cgi?id=8585},
volume = {17},
year = {2009}
}

@article{Li2015,
abstract = {Traditional methods to tackle many music information retrieval tasks typically follow a two-step architecture: feature engineering followed by a simple learning algorithm. In these "shallow" architectures, feature engineering and learning are typically disjoint and unrelated. Additionally, feature engineering is difficult, and typically depends on extensive domain expertise. In this paper, we present an application of convolutional neural networks for the task of automatic musical instrument identification. In this model, feature extraction and learning algorithms are trained together in an end-to-end fashion. We show that a convolutional neural network trained on raw audio can achieve performance surpassing traditional methods that rely on hand-crafted features.},
archivePrefix = {arXiv},
arxivId = {1511.05520},
author = {Li, Peter and Qian, Jiyuan and Wang, Tian},
eprint = {1511.05520},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/fedeeda2182eef200a4ac9af9caf95708bd0ecca.pdf:pdf},
title = {{Automatic Instrument Recognition in Polyphonic Music Using Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1511.05520},
year = {2015}
}

@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v5},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v5},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/cf5cf758f1b00214d70e34f8a57813557dc03e17.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--13},
title = {{Adam: a Method for Stochastic Optimization}},
url = {http://arxiv.org/pdf/1412.6980v8.pdf},
year = {2015}
}

@misc{McFee2015,
  author       = {Brian McFee and
                  Matt McVicar and
                  Colin Raffel and
                  Dawen Liang and
                  Oriol Nieto and
                  Eric Battenberg and
                  Josh Moore and
                  Dan Ellis and
                  Ryuichi Yamamoto and
                  Rachel Bittner and
                  Douglas Repetto and
                  Petr Viktorin and
                  Joï¿½o Felipe Santos and
                  Adrian Holovaty},
  title        = {librosa: 0.4.1. Zenodo. 10.5281/zenodo.18369},
  month        = oct,
  year         = 2015,
  doi          = {10.5281/zenodo.32193},
  url          = {http://dx.doi.org/10.5281/zenodo.32193}
}
