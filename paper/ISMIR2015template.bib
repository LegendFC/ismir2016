@article{Bittner2014,
abstract = {We introduce MedleyDB: a dataset of annotated, royalty-free multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research.},
author = {Bittner, Rachel and Salamon, Justin and Tierney, Mike and Mauch, Matthias and Cannam, Chris and Bello, Juan},
journal = {International Society for Music Information Retrieval Conference},
title = {MedleyDB: a multitrack dataset for annotation-intensive MIR research},
url = {http://marl.smusic.nyu.edu/medleydb{\_}webfiles/bittner{\_}medleydb{\_}ismir2014.pdf},
year = {2014}
}

@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.6980v5},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {arXiv:1412.6980v5},
file = {:Users/vlostan/Library/Application Support/Mendeley Desktop/Downloaded/cf5cf758f1b00214d70e34f8a57813557dc03e17.pdf:pdf},
journal = {International Conference on Learning Representations},
pages = {1--13},
title = {{Adam: a Method for Stochastic Optimization}},
url = {http://arxiv.org/pdf/1412.6980v8.pdf},
year = {2015}
}

@misc{McFee2015,
  author       = {Brian McFee and
                  Matt McVicar and
                  Colin Raffel and
                  Dawen Liang and
                  Oriol Nieto and
                  Eric Battenberg and
                  Josh Moore and
                  Dan Ellis and
                  Ryuichi Yamamoto and
                  Rachel Bittner and
                  Douglas Repetto and
                  Petr Viktorin and
                  João Felipe Santos and
                  Adrian Holovaty},
  title        = {librosa: 0.4.1. Zenodo. 10.5281/zenodo.18369},
  month        = oct,
  year         = 2015,
  doi          = {10.5281/zenodo.32193},
  url          = {http://dx.doi.org/10.5281/zenodo.32193}
}