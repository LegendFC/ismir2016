% -----------------------------------------------
% Template for ISMIR Papers
% 2015 version, based on previous ISMIR templates
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir,amsmath,cite}
\usepackage{graphicx}
\usepackage{color}
\usepackage{mathrsfs}

% Title.
% ------
\title{Learning invariants for polyphonic instrument recognition}


% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
The abstract should be placed at the top left column and should contain about 150-200 words.
\end{abstract}
%

\section{Introduction}\label{sec:introduction}
% On music instrument classification
% Ref to Joder et al
% Ref to Fuhrmann

% On feature learning
% Ref to Dieleman and Benjamin ICASSP 2014
% Ref to Humphrey, Bello, LeCun 2012
% Ref to Salamon and Bello
% Ref to Li, Qian, and Wang arXiv 2015

\section{Deep convolutional networks}
\subsection{Time-frequency representation}
We used the implementation from the librosa package \cite{McFee2015} with $Q=12$ filters per octave, center frequencies ranging from 55 Hz to 14 kHz (8 octaves from A1 to A9), and a hop size of 23 ms. Furthermore, we applied perceptual weighting of loudness in order to reduce the dynamic range between the fundamental partial and its upper harmonics. A 3-second sound excerpt $x(t)$ is represented by a time-frequency matrix $\boldsymbol{x_1}(t,k_1)$ of width $T=128$ samples and height $K_1=96$ MIDI indices.

\subsection{Architecture}
First of all, we apply a family $\boldsymbol{W_2}(t,k_1,k_2)$ of $K_2=50$ learned time-frequency convolutional operators.
Furthermore, element-wise biases $\boldsymbol{b_2}(t,k_1,k_2)$ are added to the convolutions, resulting in the tensor 

\begin{equation}
\boldsymbol{y_2}(t,k_1,k_2) =
\boldsymbol{b_2} + 
(\boldsymbol{x_1}
\overset{t,k_1}{\ast}
\boldsymbol{W_2}).
\end{equation}
 
The second step in the network is the application of a pointwise nonlinearity. We have chosen the \emph{rectified linear unit} (ReLU) because of its popularity in computer vision and its computational efficiency.
 
 \begin{equation}
 \boldsymbol{y_{2}^{+}}(t,k_1,k_2) = \max \left( \boldsymbol{y_2}(t,k_1,k_2), 0\right)
 \end{equation}
 
 
 
 \begin{equation}
 \boldsymbol{x_2}(t,k_1,k_2) =
 \max_{
\substack{
\vert \tau \vert \leq \Delta t \\
 \vert \kappa_1 \vert \leq \Delta k_1}
 } \left\{
 \boldsymbol{y_{2}^{+}}(t + \tau, k_1 + \kappa_1, k_2)
 \right\}
 \end{equation}
 
 \begin{equation}
 \boldsymbol{y_3}(t,k_1,k_3) =
 \sum_{k_2}
 (\boldsymbol{x_2}
 \overset{t,k_1}{\ast}
 \boldsymbol{W_3})
 \end{equation}

\begin{equation}
\boldsymbol{x_4}(k_4) =
\left(
\sum_{v_3}
\boldsymbol{W_4}(k_4, v_3)
\boldsymbol{x_3}(v_3) \right)^{+}
\end{equation}

\begin{equation}
\boldsymbol{x_5}(k_5) =
\left(
\sum_{k_4}
\boldsymbol{W_5}(k_5, k_4)
\boldsymbol{x_4}(k_4)
\right)^{+}
\end{equation}

\begin{equation}
\boldsymbol{x_6}(k_6) =
\sigma \left(
\sum_{k_6}
\boldsymbol{W_6}(k_6,k_5)
\boldsymbol{x_5}(k_5)
\right)
\end{equation}


% Explain how pooling works
% Make a figure

\subsection{Training}
% Random crops
% Categorical cross-entropy
The network is trained on categorical cross-entropy with \emph{Adam} \cite{Kingma2015}, a state-of-the-art stochastic optimizer for gradient-based learning.
% Adam optimizer
% Shuffled examples with uniform class distribution
% Mini-batch learning
% Dropout

\section{Deep supervision of melodic contour}
\subsection{Disentangling pitch from timbre}
% Source-filter equation
% Ref to LeCun on disentangling factors of variability
% Ref to deeply supervised nets
% Ref to NIPS 2015
% Ref to Mallat 2016

\subsection{Extraneous supervision}
% Equation that sums over k2
% Make a figure

\subsection{Joint supervision}
% Different equation
% Comment extraneous vs joint
% Visualization to compare learned filters


\section{Single-instrument classification}\label{sec:single-instrument}
\subsection{Experimental design}
In order to evaluate the proposed algorithms, we used MedleyDB \cite{Bittner2014}, a dataset of 122 multitracks annotated with instrument activations as well as melodic $f_0$ curves when present. 

\subsection{Results}


\section{Polyphonic classification}\label{sec:polyphonic}
\subsection{Experimental design}

\subsection{Results}


\section{Conclusions}

% For bibtex users:
\bibliography{ISMIR2015template}

\end{document}
